"""Answer FAQ with RAG use case."""

import re
from typing import Optional

from app.application.dtos.knowledge import KnowledgeChunk
from app.application.ports.knowledge_base_repository import KnowledgeBaseRepository
from app.application.ports.llm_client import LLMClient
from app.application.use_cases.rag_answer_formatter import RagAnswerFormatter
from app.infrastructure.config.settings import settings


class AnswerFaqWithRag:
    """Use case for answering FAQ questions using RAG."""

    # Conservative threshold for minimum score to consider chunks relevant
    MIN_SCORE_THRESHOLD = 0.1

    # Safe fallback message in Spanish
    FALLBACK_MESSAGE = (
        "No tengo esa información con certeza en este momento. "
        "¿Quieres que te ayude con opciones de autos o con un estimado de financiamiento?"
    )

    def __init__(
        self,
        knowledge_base_repository: KnowledgeBaseRepository,
        llm_client: Optional[LLMClient] = None,
    ) -> None:
        """
        Initialize FAQ RAG use case.

        Args:
            knowledge_base_repository: Repository for knowledge base retrieval
            llm_client: Optional LLM client for natural language generation
        """
        self._knowledge_base_repository = knowledge_base_repository
        self._llm_client = llm_client

    def execute(self, query: str) -> tuple[str, list[str]]:
        """
        Answer FAQ question using RAG.

        Args:
            query: User query

        Returns:
            Tuple of (reply, suggested_questions) - both in Spanish
        """
        # Retrieve relevant chunks
        chunks = self._knowledge_base_repository.retrieve(query, top_k=5)

        # Check if we have sufficient evidence
        if not chunks or (chunks and chunks[0].score < self.MIN_SCORE_THRESHOLD):
            return self._generate_fallback_response()

        # Generate answer from retrieved chunks
        reply = self._generate_answer(chunks)
        suggested_questions = self._generate_suggested_questions()

        return reply, suggested_questions

    def _generate_answer(self, chunks: list[KnowledgeChunk]) -> str:
        """
        Generate Spanish answer from retrieved chunks.

        Args:
            chunks: Retrieved knowledge chunks

        Returns:
            Spanish answer based on retrieved content
        """
        # Try LLM generation if enabled and client available
        if settings.llm_enabled and self._llm_client:
            try:
                return self._generate_answer_with_llm(chunks)
            except Exception:
                # Fallback to deterministic formatting on any error
                pass

        # Deterministic fallback: use top chunk for answer (most relevant)
        top_chunk = chunks[0]

        # Build combined text from chunks for formatting
        combined_text = top_chunk.text

        # If we have multiple relevant chunks, include them for better context
        if len(chunks) > 1 and chunks[1].score >= self.MIN_SCORE_THRESHOLD:
            combined_text += f"\n\n{chunks[1].text}"

        # Format using the human-friendly formatter
        answer = RagAnswerFormatter.format(combined_text)

        return answer

    def _generate_answer_with_llm(self, chunks: list[KnowledgeChunk]) -> str:
        """
        Generate Spanish answer using LLM to rephrase retrieved chunks.

        Args:
            chunks: Retrieved knowledge chunks

        Returns:
            Spanish answer generated by LLM based on retrieved content

        Raises:
            Exception: If LLM call fails
        """
        # Build context from chunks (grounding material)
        top_chunk = chunks[0]
        context_text = top_chunk.text

        # Include additional relevant chunks if available
        if len(chunks) > 1 and chunks[1].score >= self.MIN_SCORE_THRESHOLD:
            context_text += f"\n\nInformación adicional:\n{chunks[1].text}"

        # Clean context text (remove markdown formatting)
        context_text = re.sub(r"^#{1,6}\s+", "", context_text, flags=re.MULTILINE)
        context_text = re.sub(r"^---+\s*$", "", context_text, flags=re.MULTILINE)
        context_text = context_text.strip()

        # Create system prompt that enforces Spanish and prevents hallucination
        system_prompt = (
            "You are a helpful assistant for Kavak, a used car sales platform. "
            "Your task is to rephrase the provided knowledge base content into a natural Spanish response. "
            "CRITICAL RULES: "
            "1. Answer ONLY in Spanish. "
            "2. Use ONLY the information provided in the context. Do not add facts not in the context. "
            "3. Keep the response concise (2-3 sentences). "
            "4. Maintain accuracy of all facts from the knowledge base."
        )

        # Create user message with context
        user_message = (
            f"Based on the following information from Kavak's knowledge base, "
            f"provide a helpful Spanish answer:\n\n{context_text}"
        )

        # Call LLM with context
        reply = self._llm_client.generate_reply(
            system_prompt=system_prompt,
            user_message=user_message,
            context={"chunks": [c.text for c in chunks]},
        )

        # Format the LLM reply to ensure it's human-friendly and removes any artifacts
        # (LLM might still include some structure from the context)
        reply = RagAnswerFormatter.format(reply)

        return reply

    def _format_chunk_as_answer(self, text: str) -> str:
        """
        Format chunk text as a Spanish answer.

        Args:
            text: Chunk text (already in Spanish from curated KB)

        Returns:
            Formatted Spanish answer using KB content directly
        """
        # Use the human-friendly formatter to clean and format the text
        return RagAnswerFormatter.format(text)

    def _extract_key_point(self, text: str) -> str:
        """
        Extract a key point from chunk text for supplementary information.

        Args:
            text: Chunk text (in Spanish from curated KB)

        Returns:
            Key point in Spanish extracted from KB content, or empty string if none found
        """
        # Extract a concise key point from the Spanish KB content
        # Remove markdown and get first meaningful sentence
        text = re.sub(r"^#{1,6}\s+", "", text, flags=re.MULTILINE)
        text = text.strip()

        # Get first sentence that's not too short
        sentences = re.split(r"[.!?]\s+", text)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 30:  # Meaningful length
                return sentence

        return ""

    def _generate_fallback_response(self) -> tuple[str, list[str]]:
        """
        Generate fallback response when no relevant chunks are found.

        Returns:
            Tuple of (fallback_message, suggested_questions) - both in Spanish
        """
        suggested_questions = [
            "¿Qué autos tienen disponibles?",
            "¿Cómo funciona el financiamiento?",
            "¿Qué garantías ofrecen?",
            "¿Puedo ver el auto antes de comprarlo?",
        ]

        return self.FALLBACK_MESSAGE, suggested_questions

    def _generate_suggested_questions(self) -> list[str]:
        """
        Generate suggested questions in Spanish.

        Returns:
            List of suggested questions
        """
        return [
            "¿Qué garantías ofrecen?",
            "¿Cómo funciona la entrega?",
            "¿Puedo financiar mi compra?",
            "¿Cómo es el proceso de inspección?",
        ]
